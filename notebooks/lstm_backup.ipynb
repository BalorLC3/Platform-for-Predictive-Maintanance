{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622d7704",
   "metadata": {},
   "source": [
    "This notebook is called backup due to three reasons\n",
    "1. The nature of LSTM (input, output and forget gates) is to learn from independent sequential data. Plain LSTM learns that everything is important to survive until last timestep, while using _attention_ we teach LSTM what timesteps matter most and to average them, this helps avoid the need of mean rolling windows, and results in a faster training.\n",
    "\n",
    "2. Rolling windows helps to reduce entropy per timestep and to denoise high-frequency fluctuations, but also are _cheap_ to compute and aim to a better generalization. Meanwhile using attention requires maintanance itself, as this new \"function\" may put attention when the data changes a little\n",
    "\n",
    "3. Rolling windows in attention + LSTM naturally makes the model to memorize the clipped RUL, which is a expected result.\n",
    "\n",
    "However, for production a stable, observative and easy to debug version is better. Before was seen with some experiments, that plain LSTM has lower variance, and also provides important information to the user (moving average).\n",
    "\n",
    "So in conclusion, using attention seems a good and viable option, which helps to diagnoise our data (attention describes it). But is lack of the simplicity of plain LSTM, which is more valuable in a environment where we need stability and observability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ff1cc",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Standard Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# -- Model --\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# -- Error Metrics --\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# -- Model Selection --\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35850853",
   "metadata": {},
   "source": [
    "Fix description of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154026a",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4165f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give names to the features\n",
    "index_names = ['engine', 'cycle']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_info = [ \n",
    "\"(Fan inlet temperature) (◦R)\",\n",
    "\"(LPC outlet temperature) (◦R)\",\n",
    "\"(HPC outlet temperature) (◦R)\",\n",
    "\"(LPT outlet temperature) (◦R)\",\n",
    "\"(Fan inlet Pressure) (psia)\",\n",
    "\"(bypass-duct pressure) (psia)\",\n",
    "\"(HPC outlet pressure) (psia)\",\n",
    "\"(Physical fan speed) (rpm)\",\n",
    "\"(Physical core speed) (rpm)\",\n",
    "\"(Engine pressure ratio(P50/P2)\",\n",
    "\"(HPC outlet Static pressure) (psia)\",\n",
    "\"(Ratio of fuel flow to Ps30) (pps/psia)\",\n",
    "\"(Corrected fan speed) (rpm)\",\n",
    "\"(Corrected core speed) (rpm)\",\n",
    "\"(Bypass Ratio) \",\n",
    "\"(Burner fuel-air ratio)\",\n",
    "\"(Bleed Enthalpy)\",\n",
    "\"(Required fan speed)\",\n",
    "\"(Required fan conversion speed)\",\n",
    "\"(High-pressure turbines Cool air flow)\",\n",
    "\"(Low-pressure turbines Cool air flow)\" \n",
    "]\n",
    "\n",
    "def name_sensor_data(sensor_info: list):\n",
    "    '''Helper to describe sensor data and get column names'''\n",
    "    sensor_dict = {}\n",
    "    for i, sensor in enumerate(sensor_info):\n",
    "        sensor_dict['s_'+ str(i + 1)] = sensor\n",
    "\n",
    "    sensor_names = list(sensor_dict.keys())\n",
    "    col_names = index_names + setting_names + sensor_names\n",
    "    return sensor_dict, col_names\n",
    "\n",
    "sensor_description, col_names = name_sensor_data(sensor_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"..\") / \"data\" / \"CMaps\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(data_path / \"train_FD001.txt\",\n",
    "                       sep=r'\\s+', header=None, index_col=False, names=col_names)\n",
    "df_test = pd.read_csv( data_path / \"test_FD001.txt\",\n",
    "                      sep=r'\\s+', header=None, index_col=False, names=col_names)\n",
    "y_valid = pd.read_csv(data_path /\"RUL_FD001.txt\",\n",
    "                     sep=r'\\s+', header=None, names=['RUL'])\n",
    "print(f\"Test Shape: {df_test.shape}\\nTrain Shape:{df_train.shape}\\nRUL Labels: {y_valid.shape}\")\n",
    "print(f\"Percentage of the test-set: {len(df_test)/(len(df_train)+len(df_test))*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8269c5",
   "metadata": {},
   "source": [
    "### Detect nulls and add RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89557cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_null(dfs: list):\n",
    "    results = []\n",
    "    for i, df in enumerate(dfs):\n",
    "        null_mask = df.isnull().any(axis=1)\n",
    "        null_row_idx = np.where(null_mask)[0]\n",
    "        null_counts = df.isnull().sum(axis=1)\n",
    "        results.append((i, null_row_idx, null_mask, null_counts))\n",
    "        print(f\"Dataframe {i}:\")\n",
    "        print(f\"|   Total tows with nulls: {len(null_row_idx)}\")\n",
    "        print(f\"|   Total null values {df.isnull().sum().sum()}\")\n",
    "        if len(null_row_idx) > 0:\n",
    "            print(f\"  Rows with nulls: {null_row_idx.tolist()[:10]}\")  \n",
    "            if len(null_row_idx) > 10:\n",
    "                print(f\"  ... and {len(null_row_idx) - 10} more\")\n",
    "        else:\n",
    "            print(f\"|-->No null values detected!\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "null_report = detect_null([df_train, df_test, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_RUL(\n",
    "        df, \n",
    "        unit_col='engine', \n",
    "        cycle_col='cycle', \n",
    "        rul_col='RUL', \n",
    "        clip_max=None\n",
    "    ):\n",
    "    max_cycles = df.groupby(unit_col)[cycle_col].transform('max')\n",
    "    df[rul_col] = max_cycles - df[cycle_col]\n",
    "    if clip_max is not None:\n",
    "        # 1. clipping helps the model to have a narrower prediction space, \n",
    "        # using clip(upper) we hope the model gets a high value but at the same time does not overstimate the RUL\n",
    "        # i.e., a machine could fail if we expect it to fail at 200 but fails at 190 for some reason\n",
    "        # so we are telling the model to expect less, like a quantile regression\n",
    "        # https://www.kaggle.com/code/wassimderbel/nasa-predictive-maintenance-rul\n",
    "        df[rul_col] = df[rul_col].clip(upper=clip_max) # \n",
    "    return df\n",
    "\n",
    "df_train = add_RUL(df_train, clip_max=130)\n",
    "df_train[['cycle','RUL']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f31cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569c015",
   "metadata": {},
   "source": [
    "Point 1 is to be observed, the mean RUL is dependent of the clipping, but usually is not expected to have a high RUL (75\\% of the data is at most 155), even if clipping is removed, the mean is not high, so by configurating this parameter we can enhance the learning capability of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUL_column = df_train.RUL\n",
    "mean_rul = RUL_column.mean()\n",
    "std_rul = RUL_column.std()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "counts, bins, _ = plt.hist(df_train['RUL'], bins='rice', color=\"#159099\")\n",
    "plt.vlines(mean_rul, ymin=0, ymax=max(counts), color='r')\n",
    "plt.vlines([mean_rul - std_rul, mean_rul + std_rul], ymin=0, ymax=max(counts), \n",
    "           color='black', linestyles='dashed')\n",
    "plt.show()\n",
    "\n",
    "print(RUL_column.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be933ec5",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe04189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "def evaluate(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    label: str = ''\n",
    "):\n",
    "    \"\"\"Evaluation function\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    pp({\n",
    "        \"label\": label,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "def plot_performance(\n",
    "        y_true: torch.Tensor, \n",
    "        y_pred: torch.Tensor,\n",
    "        max_engines: int = None,\n",
    "        figsize: tuple = ((25,10))\n",
    "    ):\n",
    "    '''Visualization of predicted vs actual RUL'''\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    y_true_full = np.array(y_true).flatten()\n",
    "    y_pred_full = np.array(y_pred).flatten()\n",
    "\n",
    "    if max_engines is not None:\n",
    "        y_true = y_true_full[:max_engines].astype(float)\n",
    "        y_pred = y_pred_full[:max_engines].astype(float)\n",
    "\n",
    "\n",
    "    indices = np.arange(len(y_true))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    ax.bar(\n",
    "        x=indices - bar_width / 2, \n",
    "        height=y_true,\n",
    "        width=bar_width,\n",
    "        color='#2ecc71',\n",
    "        alpha=0.8,\n",
    "        label='[True] RUL'\n",
    "    )\n",
    "    ax.bar(\n",
    "        x=indices + bar_width / 2,\n",
    "        height=y_pred,\n",
    "        width=bar_width,\n",
    "        color='#e74c3c',\n",
    "        alpha = 0.8,\n",
    "        label='[Pred] RUL',\n",
    "    )\n",
    "    \n",
    "    # Showtime\n",
    "    for i, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
    "        ax.plot(\n",
    "            [i - bar_width / 2, i + bar_width / 2],\n",
    "            [true, pred],\n",
    "            color='#34495e',\n",
    "            ls=':',\n",
    "            alpha=0.6\n",
    "        )\n",
    "        \n",
    "    ax.set_title('RUL Prediction Performance', fontsize=20, pad=20)\n",
    "    ax.set_xlabel('Engine Index', fontsize=16)\n",
    "    ax.set_ylabel('RUL (cycles)', fontsize=16)\n",
    "    ax.set_xticks(indices)\n",
    "    ax.legend(framealpha=1,)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745192",
   "metadata": {},
   "source": [
    "## Clean data, no useless sensors \n",
    "At first, I tried to use a LSTM with (256, 256) hidden units, the problem was that it needed a rolling mean to get good results, that is because the previous architecture used a last-head strategy, in which only last information was considered for training. Using attention this need is avoided because attention provides importance to pikes or low signals that provide more information during training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"setting_1\",\n",
    "    \"setting_2\",\n",
    "    \"setting_3\",\n",
    "    \"cycle\",\n",
    "    \"s_1\", \"s_5\", \"s_6\",\"s_10\", \"s_16\", \"s_18\", \"s_19\"\n",
    "]\n",
    "\n",
    "df_clean = df_train.drop(columns=drop_cols)\n",
    "feature_columns = [col for col in df_clean.columns]\n",
    "\n",
    "def calculate_rolling_mean(df, window=1):\n",
    "    '''Calculates rolling features'''\n",
    "    df = df.copy()\n",
    "\n",
    "    sensor_cols = [c for c in df.columns if c.startswith(\"s_\")]\n",
    "\n",
    "    if window > 1:\n",
    "        for sensor in sensor_cols:\n",
    "            df[f\"{sensor}_rm\"] = (\n",
    "                df.groupby(\"engine\")[sensor]\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "df_rm = calculate_rolling_mean(df_clean)\n",
    "if any(col.endswith('_rm') for col in feature_columns):\n",
    "    sample_engine = df_rm[df_rm['engine'] == 1]\n",
    "    sample_engine[['s_17', 's_17_rm']].plot(figsize=(12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ed151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rm.drop(['engine'], axis=1, inplace=True)\n",
    "X_clean = df_rm.drop([\"RUL\"], axis=1)\n",
    "y_clean = df_rm[\"RUL\"]\n",
    "feature_columns = list(X_clean.columns)\n",
    "input_dim = len(feature_columns)\n",
    "print(f\"Input dim: {input_dim} \\n Feature Columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed38fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sequences(\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        SEQ_LEN: int\n",
    "    ):\n",
    "    X_np = np.array(X)\n",
    "    y_np = np.array(y)\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_np) - SEQ_LEN):\n",
    "        X_seq.append(X_np[i:i+SEQ_LEN])\n",
    "        y_seq.append(y_np[i+SEQ_LEN])\n",
    "    X_seq = torch.tensor(X_seq, dtype=torch.float32)\n",
    "    y_seq = torch.tensor(y_seq, dtype=torch.float32).unsqueeze(1)\n",
    "    return X_seq, y_seq\n",
    "\n",
    "SEQ_LEN = 30\n",
    "X_seq, y_seq = _get_sequences(\n",
    "    X_clean,\n",
    "    y_clean,\n",
    "    SEQ_LEN=SEQ_LEN\n",
    ")\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_seq, y_seq, shuffle=False\n",
    ")\n",
    "\n",
    "mean = X_train_seq.mean(dim=(0, 1), keepdim=True)  # (1, 1, F)\n",
    "std  = X_train_seq.std(dim=(0, 1), keepdim=True) + 1e-8\n",
    "X_train_seq = (X_train_seq - mean) / std\n",
    "X_test_seq  = (X_test_seq  - mean) / std\n",
    "\n",
    "def arr_to_tensor(data: np.ndarray | pd.DataFrame):\n",
    "    '''Converts from pd.DataFrame/np.ndarray to torch.tensor'''\n",
    "    if not isinstance(data, (pd.DataFrame, np.ndarray)):\n",
    "        raise TypeError(\"data can't be converted to tensor\")\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data, dtype=torch.float32)\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return torch.tensor(data.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "def get_device(device: torch.device | str = \"auto\") -> torch.device:\n",
    "    \"\"\"\n",
    "    :param device: One for 'auto', 'cuda', 'cpu'\n",
    "    :return: supported PyTorch device\n",
    "    \"\"\"\n",
    "    if device == \"auto\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.device(device)\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2, # fast and secure maintanance n_layers > 3 consumes more time\n",
    "        output_dim: int = 1,\n",
    "        dropout_prob: float = 0.3,\n",
    "        device: torch.device | str = \"auto\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.device = get_device(device)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout_prob if self.num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "            device = self.device\n",
    "        )\n",
    "\n",
    "        self.attn_score = nn.Linear(self.hidden_dim, 1, device=self.device)\n",
    "\n",
    "        # Regression head\n",
    "        self.head = nn.Linear(self.hidden_dim, self.output_dim, device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Compute attention scores and weights\n",
    "        scores = self.attn_score(lstm_out)  # (batch, seq_len, 1)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (batch, seq_len, 1)\n",
    "\n",
    "        # Weighted sum over time\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch, hidden_dim)\n",
    "\n",
    "        # Final prediction\n",
    "        return self.head(context)\n",
    "\n",
    "print(f\"Training using a sequence of length {SEQ_LEN}, running in {get_device()}\")\n",
    "print(LSTMRegressor(input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55129be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X = X_seq\n",
    "        self.y = y_seq\n",
    "\n",
    "        assert self.X.dim() == 3\n",
    "        assert self.y.dim() == 2\n",
    "        assert len(self.X) == len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SequenceDataset(X_train_seq, y_train_seq)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=256,     \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "model = LSTMRegressor(input_dim=input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mean_loss = total_loss / len(train_loader)\n",
    "    history.append(mean_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"> Epoch {epoch+1}/{epochs}, loss = {mean_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723859c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history: list):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(\n",
    "        range(len(history)), \n",
    "        history,\n",
    "        color='red'\n",
    "    )\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss (MSE)')\n",
    "    ax.grid(axis='both', alpha=0.5, ls=':')\n",
    "    plt.show()\n",
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': model.input_dim,\n",
    "    'hidden_dim': model.hidden_dim,\n",
    "    'num_layers': model.num_layers,\n",
    "    'dropout_prob': model.dropout_prob, \n",
    "    'device': device,\n",
    "    'normalization_stats': {\n",
    "        'mean': mean,  \n",
    "        'std': std     \n",
    "    }\n",
    "}\n",
    "\n",
    "save_dir = Path(\"../checkpoints\")   # <- whatever folder you want\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_path = save_dir / \"lstm_model_inference_backup.pth\"\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(\"Checkpoint saved correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facae5f",
   "metadata": {},
   "source": [
    "# Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "\n",
    "# Instantiate the model using the saved hyperparameters\n",
    "model_for_testing = LSTMRegressor(\n",
    "    input_dim=checkpoint['input_dim'],\n",
    "    hidden_dim=checkpoint['hidden_dim'],\n",
    "    num_layers=checkpoint['num_layers'],\n",
    "    dropout_prob=checkpoint.get('dropout_prob', 0.2) # .get for backward compatibility\n",
    ").to('cpu')\n",
    "\n",
    "# Load the trained weights\n",
    "model_for_testing.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# This turns off dropout and other training-specific layers.\n",
    "model_for_testing.eval()\n",
    "\n",
    "print(\"Model loaded successfully from checkpoint for testing.\")\n",
    "\n",
    "# The test set `X_test_seq` is already normalized, so we can use it directly.\n",
    "# Use torch.no_grad() to make predictions more efficient (no gradients needed)\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model_for_testing(X_test_seq)\n",
    "\n",
    "# Convert predictions and ground truth to NumPy arrays for plotting\n",
    "# .cpu() is important if trained on a GPU\n",
    "y_pred_np = y_pred_tensor.cpu().numpy()\n",
    "y_true_np = y_test_seq.cpu().numpy()\n",
    "\n",
    "\n",
    "print(evaluate(y_true_np, y_pred_np, type(model).__name__))\n",
    "\n",
    "SAMPLES_TO_PLOT = 50\n",
    "\n",
    "print(f\"Plotting performance for the first {SAMPLES_TO_PLOT} test samples...\")\n",
    "\n",
    "plot_performance(\n",
    "    y_true=y_true_np, \n",
    "    y_pred=y_pred_np,\n",
    "    max_engines=SAMPLES_TO_PLOT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3719e",
   "metadata": {},
   "source": [
    "## Final valid testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f83f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 30 # The same sequence length you used for training\n",
    "df_test_rm = calculate_rolling_mean(df_test)\n",
    "feature_columns_rm = feature_columns + [col for col in df_rm if col.endswith('rm')]\n",
    "\n",
    "X_test_sequences = []\n",
    "for engine_id in df_test_rm['engine'].unique():\n",
    "    # Get the data for the current engine\n",
    "    engine_data = df_test_rm[df_test_rm['engine'] == engine_id]\n",
    "    \n",
    "    # Extract the features for that engine\n",
    "    engine_features = engine_data[feature_columns]\n",
    "    last_sequence = engine_features.tail(SEQ_LEN).to_numpy()\n",
    "    \n",
    "    X_test_sequences.append(last_sequence)\n",
    "\n",
    "X_test_final_np = np.array(X_test_sequences)\n",
    "\n",
    "X_test_final = torch.tensor(X_test_final_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"Shape of the final test sequences tensor: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a899ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "\n",
    "mean = checkpoint['normalization_stats']['mean']\n",
    "std = checkpoint['normalization_stats']['std']\n",
    "\n",
    "# Apply the normalization\n",
    "X_test_final_normalized = (X_test_final - mean) / std\n",
    "\n",
    "# Make predictions on the final, prepared test set\n",
    "with torch.no_grad():\n",
    "    y_pred_final_tensor = model_for_testing(X_test_final_normalized)\n",
    "\n",
    "# Convert predictions and ground truth to NumPy arrays\n",
    "y_pred_final_np = y_pred_final_tensor.cpu().numpy()\n",
    "y_true_final_np = y_valid.to_numpy() # y_valid is the RUL_FD001.txt data\n",
    "\n",
    "evaluate(y_true_final_np, y_pred_final_np, type(model).__name__)\n",
    "\n",
    "# Visualize the results using your plotting function\n",
    "print(\"Plotting performance on the final hold-out test set...\")\n",
    "plot_performance(\n",
    "    y_true=y_true_final_np, \n",
    "    y_pred=y_pred_final_np,\n",
    "    max_engines=SAMPLES_TO_PLOT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795a5f4",
   "metadata": {},
   "source": [
    "log: 'hidden_dim' = 128, got 0.78 of r2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vasudeva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
